# Real_time_class_room_engagement_and_interaction_monitoring_using_YOLO-V5
A real-time system that analyzes classroom engagement and student interaction using YOLOv5-based object detection and activity recognition. Includes live tracking, attention measurement, participation monitoring, and automated analytics for improving teaching effectiveness.


ğŸš€ About This Project

This project is my attempt to answer one simple question:

â€œCan a classroom understand whatâ€™s happening inside it â€” without a teacher manually observing every student?â€

Using YOLOv5, DeepSORT tracking, and a custom emotion-recognition module, this system watches a live classroom feed and tries to understand what students are doing in real time â€”
whether theyâ€™re paying attention, distracted, writing, using a phone, sleepy, engaged, or somewhere in between.

All the engagement cues are then blended into a single score that helps teachers instantly see how their class is doing.
Everything runs locally on edge devices (like Jetson Xavier NX), so no cloud, no privacy headache.

This project started as part of our academic research work, and evolved into a functional prototype that actually performs well in real classroom-like environments.

IEEE_conference_paper_for_majorâ€¦

ğŸ¯ What This System Can Do

âœ” Detect classroom activities using YOLOv5 (writing, listening, phone usage, hand-raising, sleeping, etc.)
âœ” Track each student consistently across frames with DeepSORT
âœ” Recognize emotions (happy, neutral, sad, angry, etc.) even with masked faces
âœ” Generate real-time engagement scores (0â€“100)
âœ” Display live analytics on a Streamlit dashboard
âœ” Maintain real-time performance on edge hardware (20â€“25 FPS)
âœ” Keep all data private â€” nothing leaves the device

Basically: a smart classroom assistant that doesnâ€™t interrupt the class.

ğŸ§  How It Works (Simple Explanation)

Camera Feed In â†’ Video enters the system frame by frame.

YOLOv5 Detection â†’ Students + actions are detected.

DeepSORT Tracking â†’ Each student gets a unique ID to track over time.

Emotion Classifier â†’ Cropped faces are analyzed for emotional cues.

Engagement Scoring â†’ Behaviors + emotions are combined into a weighted score.

Visualization â†’ A Streamlit dashboard shows attention levels, trends, and alerts.

All this happens continuously, in real time.

ğŸ“Š Engagement Scoring (Human Explanation)

Instead of guessing whoâ€™s paying attention, the system calculates it.

Writing â†’ +20

Fully focused â†’ +20

Happy/Interested â†’ +10

Neutral â†’ 0

Sad/Bored â†’ â€“20

Sleeping â†’ â€“50

Phone usage â†’ â€“10

These values come from our research-based design.

IEEE_conference_paper_for_majorâ€¦


The final number (0â€“100) shows how engaged a student or the entire class is.

ğŸ–¥ï¸ Tech Stack
Core Models

YOLOv5 (custom trained)

DeepSORT for multi-object tracking

CNN-based emotion recognition

Backend & Utilities

PyTorch

OpenCV

NumPy / Pandas

FilterPy

SciPy

Streamlit (for dashboard)
